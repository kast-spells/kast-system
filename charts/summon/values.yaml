################################################################################
# Summon - Kubernetes Workload Deployment Chart
################################################################################
# Summon is a flexible Helm chart for deploying containerized workloads on
# Kubernetes. It supports 5 workload types and includes comprehensive features
# for production deployments.
#
# Supported Workload Types:
#   - Deployment:  Stateless applications with rolling updates
#   - StatefulSet: Stateful applications with persistent identity
#   - Job:         One-time batch processing tasks
#   - CronJob:     Scheduled recurring jobs
#   - DaemonSet:   One pod per node (monitoring, log collectors)
#
# Examples: See charts/summon/examples/ for comprehensive usage examples
################################################################################

################################################################################
# WORKLOAD CONFIGURATION
################################################################################
# Controls the type and behavior of the Kubernetes workload resource.
# The workload type determines which Kubernetes resource is created.
workload:
  enabled: true         # Create workload resource (Deployment/StatefulSet/Job/etc.)
  type: deployment      # Options: deployment, statefulset, job, cronjob, daemonset
  replicas: 1           # Number of pod replicas (ignored for daemonset and jobs)

  # StatefulSet-specific: Volume Claim Templates
  # Creates persistent volumes that follow pod lifecycle
  # Example: See charts/summon/examples/statefulset-with-storage.yaml
  # volumeClaimTemplates:
  #   data-volume:
  #     destinationPath: /data
  #     size: 10Gi
  #     storageClass: fast-ssd  # Optional, uses cluster default if not specified
  #     accessModes: ReadWriteOnce  # Optional, default: ReadWriteOnce
  #   cache-volume:
  #     destinationPath: /cache
  #     size: 5Gi

  # CronJob-specific: Schedule (REQUIRED for cronjob type)
  # Cron format: minute hour day month weekday
  # schedule: "0 3 * * *"      # Run daily at 3 AM
  # schedule: "*/5 * * * *"    # Run every 5 minutes
  # schedule: "0 */2 * * *"    # Run every 2 hours

  # Job/CronJob-specific: Retry and timeout configuration
  # backoffLimit: 3            # Number of retries before marking job as failed
  # activeDeadlineSeconds: 600 # Maximum time in seconds for job to complete

################################################################################
# CONTAINER IMAGE
################################################################################
# Specifies the container image to deploy
image:
  repository: ""       # Image repository (e.g., nginx, myorg/app, ghcr.io/org/app)
  pullPolicy: IfNotPresent  # Options: Always, IfNotPresent, Never
  name: ""             # Optional: Custom container name (defaults to chart name)
  tag: ""              # Optional: Image tag (overrides chart appVersion if specified)

# Secrets for pulling images from private registries
imagePullSecrets: []
# - name: regcred
# - name: dockerhub-secret

################################################################################
# RESOURCE NAMING
################################################################################
# Override default naming behavior
nameOverride: ""      # Override the chart name in resource names
fullnameOverride: ""  # Override the full generated name (chart + release)

################################################################################
# SERVICE ACCOUNT
################################################################################
# Kubernetes ServiceAccount for pod identity and RBAC
serviceAccount:
  enabled: true       # Create a ServiceAccount for the workload
  automount: true     # Automatically mount ServiceAccount token in pods
  annotations: {}     # Annotations for the ServiceAccount (e.g., IAM roles)
  # vault.hashicorp.com/role: my-app
  # eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/my-role
  labels: {}          # Additional labels for the ServiceAccount
  name: ""            # Custom ServiceAccount name (auto-generated if empty)

################################################################################
# POD CONFIGURATION
################################################################################
# Pod-level annotations and labels

# Annotations applied to pod template (e.g., for monitoring, service mesh)
podAnnotations: {}
# prometheus.io/scrape: "true"
# prometheus.io/port: "8080"
# prometheus.io/path: "/metrics"

# Additional labels applied to pod template
podLabels: {}
# version: "1.0.0"
# environment: "production"

################################################################################
# SECURITY CONTEXT
################################################################################
# Container-level security settings
# NOTE: Pod-level security context (podSecurityContext) is not implemented
securityContext: {}
# capabilities:
#   drop:
#   - ALL
#   add:
#   - NET_BIND_SERVICE
# readOnlyRootFilesystem: true
# runAsNonRoot: true
# runAsUser: 1000
# allowPrivilegeEscalation: false

################################################################################
# SERVICE
################################################################################
# Kubernetes Service for network access to pods
service:
  enabled: true       # Create a Service resource
  type: ClusterIP     # Options: ClusterIP, NodePort, LoadBalancer
  annotations: {}     # Service annotations (e.g., for load balancers)
  # service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
  # cloud.google.com/neg: '{"ingress": true}'
  labels: {}          # Additional labels for the Service
  ports: []           # List of ports to expose
  # - port: 80            # Service port
  #   targetPort: 8080    # Container port (defaults to port if not specified)
  #   protocol: TCP       # Protocol: TCP or UDP
  #   name: http          # Port name (used by Istio, monitoring)
  # - port: 443
  #   targetPort: 8443
  #   protocol: TCP
  #   name: https
  #   nodePort: 30443     # Static node port (NodePort type only)

################################################################################
# RESOURCES
################################################################################
# CPU and memory limits/requests for the container
resources: {}
# limits:
#   cpu: 500m         # Maximum CPU (0.5 cores)
#   memory: 512Mi     # Maximum memory
# requests:
#   cpu: 250m         # Requested CPU (0.25 cores)
#   memory: 256Mi     # Requested memory

################################################################################
# HEALTH CHECKS
################################################################################
# Liveness, readiness, and startup probes for container health monitoring
# Example: See charts/summon/examples/complex-production.yaml
# probes:
#   liveness:
#     type: httpGet              # Probe type: httpGet, tcpSocket, exec
#     path: /healthz             # HTTP path to check
#     port: 8080                 # Port to probe
#     initialDelaySeconds: 30    # Wait before first probe
#     periodSeconds: 10          # Interval between probes
#     timeoutSeconds: 5          # Probe timeout
#     successThreshold: 1        # Consecutive successes to mark healthy
#     failureThreshold: 3        # Consecutive failures to mark unhealthy
#     httpHeaders:
#     - name: X-Custom-Header
#       value: "HealthCheck"
#   readiness:
#     type: tcpSocket            # Check if port is open
#     port: 8080
#     initialDelaySeconds: 5
#     periodSeconds: 5
#     timeoutSeconds: 1
#     successThreshold: 1
#     failureThreshold: 3
#   startup:
#     type: exec                 # Run command in container
#     command:
#     - /bin/sh
#     - -c
#     - "test -f /app/ready"
#     initialDelaySeconds: 0
#     periodSeconds: 10
#     timeoutSeconds: 1
#     successThreshold: 1
#     failureThreshold: 30       # Allow up to 5 minutes for startup

################################################################################
# AUTOSCALING
################################################################################
# Horizontal Pod Autoscaler for automatic scaling based on metrics
autoscaling:
  enabled: false                        # Enable HPA
  minReplicas: 1                        # Minimum number of pods
  maxReplicas: 10                       # Maximum number of pods
  targetCPUUtilizationPercentage: 80    # Target CPU utilization
  # targetMemoryUtilizationPercentage: 80  # Target memory utilization
  # behavior:                           # Advanced scaling behavior
  #   scaleDown:
  #     stabilizationWindowSeconds: 300  # Wait before scaling down
  #   scaleUp:
  #     stabilizationWindowSeconds: 60   # Wait before scaling up

################################################################################
# POD SCHEDULING
################################################################################
# Control which nodes pods are scheduled on

# Node selector: Simple key-value node selection
nodeSelector: {}
# disktype: ssd
# node.kubernetes.io/instance-type: t3.large

# Tolerations: Allow pods to schedule on tainted nodes
tolerations: []
# - key: "key1"
#   operator: "Equal"
#   value: "value1"
#   effect: "NoSchedule"
# - key: "node.kubernetes.io/unreachable"
#   operator: "Exists"
#   effect: "NoExecute"
#   tolerationSeconds: 300

# Affinity: Advanced pod and node affinity rules
affinity: {}
# podAntiAffinity:                     # Spread pods across nodes
#   preferredDuringSchedulingIgnoredDuringExecution:
#   - weight: 100
#     podAffinityTerm:
#       topologyKey: kubernetes.io/hostname
#       labelSelector:
#         matchLabels:
#           app: myapp
# nodeAffinity:                        # Require specific node types
#   requiredDuringSchedulingIgnoredDuringExecution:
#     nodeSelectorTerms:
#     - matchExpressions:
#       - key: node-role.kubernetes.io/worker
#         operator: In
#         values:
#         - "true"

################################################################################
# INIT CONTAINERS
################################################################################
# Containers that run before the main container starts
# Use cases: Database migrations, wait-for-dependencies, setup tasks
initContainers: {}
# db-migration:
#   image:
#     repository: migrate/migrate
#     tag: latest
#   command:
#   - migrate
#   args:
#   - -path=/migrations
#   - -database=postgres://user:pass@db/mydb
#   - up
#   resources:
#     limits:
#       cpu: 200m
#       memory: 256Mi
# wait-for-db:
#   image:
#     repository: busybox
#     tag: latest
#   command: ['sh', '-c', 'until nc -z db 5432; do sleep 1; done']

################################################################################
# SIDECAR CONTAINERS
################################################################################
# Additional containers that run alongside the main container
# Use cases: Logging, monitoring, proxies, adapters
sideCars: {}
# prometheus-exporter:
#   image:
#     repository: prom/node-exporter
#     tag: latest
#   resources:
#     limits:
#       cpu: 100m
#       memory: 128Mi
# cloud-sql-proxy:
#   image:
#     repository: gcr.io/cloudsql-docker/gce-proxy
#     tag: latest
#   command:
#   - /cloud_sql_proxy
#   - -instances=project:region:instance=tcp:5432

################################################################################
# SECRETS
################################################################################
# Kubernetes Secrets configuration with unified contentType system
# Secrets can be referenced (location: local) or created (location: create)
# Example: See charts/summon/examples/deployment-with-secrets-env.yaml
secrets: {}
# db-credentials:
#   contentType: env        # All keys added to container environment via envFrom
#   location: local         # Reference existing secret
# tls-cert:
#   contentType: file       # Mount secret as file
#   location: local
#   mountPath: /etc/tls
#   name: tls.crt
# api-key:
#   contentType: file
#   location: create        # Create the secret from content below
#   mountPath: /etc/api
#   content: "sk_live_abcdef123456"
# app-secrets:
#   contentType: yaml       # YAML formatted file
#   location: create
#   mountPath: /config
#   name: secrets.yaml
#   content:
#     database:
#       host: postgres.local
#       password: secretpass

################################################################################
# CONFIGMAPS
################################################################################
# Kubernetes ConfigMaps configuration with unified contentType system
# ConfigMaps can be referenced (location: local) or created (location: create)
# Example: See charts/summon/examples/configmap-unified-contenttype.yaml
configMaps: {}
# app-env-vars:
#   contentType: env        # All keys as environment variables via envFrom
#   location: local         # Reference existing ConfigMap
# app-config:
#   contentType: yaml       # YAML formatted config file
#   location: create
#   name: config.yaml
#   mountPath: /app/config
#   content:
#     server:
#       port: 8080
#       host: 0.0.0.0
#     features:
#       cache: true
#       metrics: true
# nginx-conf:
#   contentType: file       # Plain text file
#   location: create
#   name: nginx.conf
#   mountPath: /etc/nginx
#   content: |
#     server {
#       listen 80;
#       location / {
#         proxy_pass http://backend;
#       }
#     }
# app-settings:
#   contentType: json       # JSON formatted file
#   location: create
#   name: settings.json
#   mountPath: /app/settings
#   content:
#     debug: false
#     timeout: 30
#     retries: 3

################################################################################
# VOLUMES
################################################################################
# Persistent and ephemeral storage volumes
# Supports: PVC, hostPath, NFS, emptyDir
# Example: See charts/summon/examples/deployment-with-storage.yaml
volumes: {}
# # PersistentVolumeClaim - reference existing PVC
# data-existing:
#   type: pvc
#   destinationPath: /data
#   pvcName: existing-pvc-name
#   readOnly: false
#
# # PersistentVolumeClaim - create new PVC
# data-new:
#   type: pvc
#   destinationPath: /var/data
#   create: true
#   size: 10Gi
#   storageClass: fast-ssd
#   accessMode: ReadWriteOnce
#
# # HostPath volume - mount path from node
# host-data:
#   type: hostPath
#   destinationPath: /host/data
#   path: /var/lib/data
#
# # NFS volume - network file system
# shared-data:
#   type: nfs
#   destinationPath: /mnt/nfs
#   server: nfs.example.com
#   path: /exports/data
#   readOnly: false
#
# # EmptyDir volume - temporary storage (deleted when pod stops)
# temp-data:
#   type: emptyDir
#   destinationPath: /tmp/data
#   sizeLimit: 1Gi
#   medium: Memory  # Optional: Use tmpfs (RAM) instead of disk

################################################################################
# ENVIRONMENT VARIABLES
################################################################################
# Container environment variables with multiple sources
envs: {}
# # Simple key-value pairs
# LOG_LEVEL: debug
# APP_ENV: production
#
# # From Secret
# DB_PASSWORD:
#   type: secret
#   name: db-secret
#   key: password
#
# # From ConfigMap
# API_ENDPOINT:
#   type: configmap
#   name: app-config
#   key: api.endpoint

################################################################################
# CONTAINER LIFECYCLE
################################################################################
# Override container command and arguments
# command: ["nginx"]
# args: ["-g", "daemon off;"]

# Lifecycle hooks for container startup and shutdown
# lifecycle:
#   preStop:                # Run before container stops (graceful shutdown)
#     exec:
#       command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
#   postStart:              # Run after container starts
#     httpGet:
#       path: /healthz
#       port: 8080


################################################################################
# KUBERNETES LABELS
################################################################################
# Optional label systems for cost tracking and identity management

# FinOps Labels: Cost allocation and tracking (Kubecost-compatible)
# Enable for cloud cost tracking, chargeback/showback, and resource analytics
# Example: See charts/summon/examples/deployment-with-finops-labels.yaml
# labels:
#   finops:
#     enabled: false        # Set to true to enable FinOps labels
#     team: ""              # Team responsible (e.g., platform-team)
#     owner: ""             # Resource owner - email/username (e.g., john.doe)
#     costCenter: ""        # Cost center for chargeback (e.g., engineering-dept)
#     department: ""        # Department (e.g., engineering, finance, platform)
#     project: ""           # Project for cost allocation (e.g., core-api)
#     environment: ""       # Environment (defaults to chapter.name if not set)
#
#   # Covenant Labels: Identity & Access Management tracking
#   # Used for tracking IAM resources, users, teams, and organizational structure
#   covenant:
#     enabled: false        # Set to true to enable Covenant labels
#     team: ""              # Team name
#     owner: ""             # Owner email
#     department: ""        # Department
#     member: ""            # Member identifier (username/service account)
#     organization: ""      # Organization name

################################################################################
# KAST SYSTEM CONFIGURATION
################################################################################
# Kast-specific configuration for GitOps integration and labeling

# Spellbook: Top-level grouping (platform/project name)
# Used for app.kubernetes.io/part-of label and infrastructure tracking
spellbook:
  name: default

# Chapter: Environment/stage grouping (dev/staging/production)
# Used for infrastructure labels and as fallback for environment label
chapter:
  name: default

# Component: Microservice categorization
# Adds app.kubernetes.io/component label for organizing related services
# component: backend    # Examples: backend, frontend, database, cache, api, worker
